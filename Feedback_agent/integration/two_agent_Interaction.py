from openai import OpenAI
import time
import sys
import os
import pandas as pd
sys.dont_write_bytecode = True

sys.path.append(os.path.abspath("../rubric_and_sample"))
import IELTS_rubrics as rubric
client = OpenAI()


generator = """
You are an IELTS writing section examiner. Given the following writing question and student essay, provide comments and feedbacks for this student to improve on the essay.
The feedback must be constructive, and provide an additional section for overall suggestions and improvements and an overall summary. Here is an IELTS Rubric for your reference.\n
"""
# For now, just use the basic rubric
generator += f"Rubric: {rubric.BASIC_RUBRIC}\n\n"

generator += """Also, there will be an evaluation for the generated feedback. Please revise the feedback according to the evaluation if the feedback is not good enough.
Below is the writing question and the student essay for feedback:
"""


# evaluator = """
# You should evaluate the generated feedback for the IELTS writing response. 
# Evaluate the quality of the feedback based on authenticity and effectiveness. 
# Specifically, consider whether the feedback is constructive, 
# addresses key aspects of writing (such as grammar, coherence, vocabulary, and task completion).
# If the feedback is good enough, just say "good" If not, give some revision suggestions. 
# Below is the writing question and the student essay for reference:
# """
evaluator = """
You should evaluate the generated feedback for the IELTS writing response. 
Evaluate the provided feedback on the IELTS essay according to these criteria: 
Clarity: Ensure the feedback is easy to understand and avoids ambiguous language. Good feedback should use simple and precise wording, so the essay writer knows exactly what is meant.
Relevance: Check if the feedback addresses the main components of IELTS scoring (Task Achievement, Coherence and Cohesion, Lexical Resource, and Grammatical Range and Accuracy). 
Good feedback covers each of these areas as relevant to the essay.
Specificity: Look for details in the feedback. Comments like 'Improve grammar' are not specific enough. 
Good feedback should mention exact issues (e.g., 'There are multiple subject-verb agreement errors') and, if possible, give examples from the essay.
Actionable Suggestions: Verify that the feedback includes clear, constructive advice that the writer can apply to improve. 
Instead of saying 'The vocabulary is weak,' a good suggestion might be, 'Try to use more varied synonyms for common words like "important" (e.g., "crucial," "vital") to improve Lexical Resource.'
Tone: Check if the feedback is encouraging and respectful. A supportive tone can help motivate the writer to make improvements.
Continue to review the feedback until all criteria are met. If the feedback meets these criteria, respond with 'good'. 
If not, identify the areas needing improvement."
Below is the writing question and the student essay for reference:
"""





def get_response(role_prompt, role, conversation_history):
    if role == 'generator':
        message = [
            {"role": "system", "content": role_prompt},
            {"role": "user", "content": f"""Based on the given conversation below, if the evaluation thinks that the feedback is good enough, repsonse with 'thank you'. 
             Otherwise,imporve the feedback generated by the generator based on the evaluator's evaluation.\nHere is the conversation:\n{conversation_history}"""}
        ]
    elif role == 'evaluator':
        message = [
            {"role": "system", "content": role_prompt},
            {"role": "user", "content": f"""Based on the given conversation below, if the generator generates a feedback, give an evaluation on the feedback generated by the generator.
             Otherwise, response with 'thank you'. Be strict and harsh on the feedback.\nHere is the conversation:\n{conversation_history}"""}
        ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=message
    )
    return response.choices[0].message.content



if __name__ == "__main__":
    # file path just used for testing
    val_path = "../../data/dataset/processed/validation.csv"
    df =  pd.read_csv(val_path, encoding='utf-8')
    df = df.iloc[[0]]
    prompt_list = df["prompt"]
    essay_list = df["essay"]
    prompt_example = prompt_list[0]
    essay_example = essay_list[0]
    generator += f"\nPrompt: {prompt_example}\n\nEssay: {essay_example}"
    evaluator += f"\nPrompt: {prompt_example}\n\nEssay: {essay_example}"

    # Let the generator start the conversation by providing a feedback
    conversation = []
    message = [
        {"role": "system", "content": generator},
        {"role": "user", "content": f"Generate the feedback."}
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=message
    )
    first_response = response.choices[0].message.content
    conversation.append(f"generator: {first_response}")
    




    # Just 3 turns for now
    for i in range(3):
        current_conversation = "\n".join(conversation)
        
        evaluator_response = get_response(evaluator,'evaluator', current_conversation)
        conversation.append(f"evaluator: {evaluator_response}")
        time.sleep(0.5)
        
        # Update conversation for generator's turn
        current_conversation = "\n".join(conversation)
        
        # Generator's turn
        generator_response = get_response(generator,'generator', current_conversation)
        conversation.append(f"generator: {generator_response}")
        time.sleep(0.5)
        
        print(f"Turn {i+1} finished")

        
    with open("conversations_log_2.txt", "w", encoding='utf-8') as f:
        for line in conversation:
            f.write(line + "\n")

    print("Finish")