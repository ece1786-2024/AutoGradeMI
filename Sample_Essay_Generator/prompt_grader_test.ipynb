{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Feedback_agent.rubric_and_sample import IELTS_rubrics as rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_path = \"./sample_essay.csv\"\n",
    "df = pd.read_csv(feedback_path, encoding='utf-8')\n",
    "\n",
    "topic = df[\"topic\"]\n",
    "essay = df[\"essay\"]\n",
    "feedback = df[\"feedback\"]\n",
    "predicted_grade = df[\"predicted\"]\n",
    "desired_grade = df[\"desired\"]\n",
    "sample_grade = df[\"sample_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_prompt_version(topic, essay):\n",
    "    client = OpenAI()\n",
    "    grader_prompt = f\"\"\"\n",
    "    You are an IELTS writing section examiner. \n",
    "    Given the writing queston and the student essay, please grade the essay on a scale of 0 to 9 based on the IELTS Rubric and 0.5 intervals are allowed.\n",
    "\n",
    "    Writing Question: {topic}\n",
    "    Student Essay: {essay}\n",
    "\n",
    "    Here is an IELTS rubric for your reference: \n",
    "    Rubric: \n",
    "    {rubric.BASIC_RUBRIC}\n",
    "    {rubric.CRITERIA}\n",
    "    {rubric.BAND_SCORE}\n",
    "\n",
    "    Please output the score of the essay in the form of 'score of the essay'. Please output the score directly.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": grader_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_with_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        if pred == '<4' or truth == '<4':\n",
    "            if pred == truth:\n",
    "                correct += 1\n",
    "        elif float(pred) == float(truth) or abs(float(pred) - float(truth)) == 0.5:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def get_acc_no_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        if pred == '<4' or truth == '<4':\n",
    "            if pred == truth:\n",
    "                correct += 1\n",
    "        elif float(pred) == float(truth):\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.0', '8.0', '6.0', '7.0', '7', '4.5', '6.0', '7.0', '5.5', '7', '7.5', '6.5', '5.0', '6.0']\n",
      "0     5.0\n",
      "1     5.5\n",
      "2     5.5\n",
      "3     5.0\n",
      "4     7.5\n",
      "5     6.5\n",
      "6     4.5\n",
      "7     7.0\n",
      "8     6.0\n",
      "9     7.0\n",
      "10    7.5\n",
      "11    7.0\n",
      "12    6.0\n",
      "13    5.5\n",
      "Name: predicted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "truth_list = predicted_grade\n",
    "pred_list = []\n",
    "\n",
    "for (q,e) in zip(topic, essay):\n",
    "    pred_list.append(get_score_prompt_version(q, e))\n",
    "\n",
    "print(pred_list)\n",
    "print(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== For predicting essay score from the dataset ====\n",
      "accuracy with no tolerace: 0.2857142857142857\n",
      "accuracy with 0.5 tolerace: 0.6428571428571429\n",
      "\n",
      " ==== For predicting sample essay score from the desired score for essays in the dataset ====\n",
      "accuracy with no tolerace: 0.5714285714285714\n",
      "accuracy with 0.5 tolerace: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==== For predicting essay score from the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(pred_list, truth_list)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(pred_list, truth_list)}\\n\")\n",
    "\n",
    "print(\" ==== For predicting sample essay score from the desired score for essays in the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(sample_grade, desired_grade)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(sample_grade, desired_grade)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
