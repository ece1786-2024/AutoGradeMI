{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Feedback_agent.rubric_and_sample import IELTS_rubrics as rubric\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_path = \"./sample_essay.csv\"\n",
    "df = pd.read_csv(feedback_path, encoding='utf-8')\n",
    "\n",
    "topic = df[\"topic\"]\n",
    "essay = df[\"essay\"]\n",
    "feedback = df[\"feedback\"]\n",
    "predicted_grade = df[\"predicted\"]\n",
    "desired_grade = df[\"desired\"]\n",
    "sample_grade = df[\"sample_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_prompt_version(topic, essay):\n",
    "    client = OpenAI()\n",
    "    grader_prompt = f\"\"\"\n",
    "    You are an IELTS writing section examiner. \n",
    "    Given the writing queston and the student essay, please grade the essay on a scale of 0 to 9 based on the IELTS Rubric and 0.5 intervals are allowed.\n",
    "\n",
    "    Writing Question: {topic}\n",
    "    Student Essay: {essay}\n",
    "\n",
    "    Here is an IELTS rubric for your reference: \n",
    "    Rubric: \n",
    "    {rubric.BASIC_RUBRIC}\n",
    "    {rubric.CRITERIA}\n",
    "    {rubric.BAND_SCORE}\n",
    "\n",
    "    Please output the score of the essay in the form of 'score of the essay'. Please output the score directly.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": grader_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"../RAG/faiss_index_train.bin\")\n",
    "with open(\"../RAG/embeddings_dataset_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cosine_similarity(query_text, top_k=3):\n",
    "    \n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding_np)\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = metadata[idx]\n",
    "        result[\"similarity\"] = distances[0][i]  \n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: '<4',\n",
    "    1: 4.0,\n",
    "    2: 4.5,\n",
    "    3: 5.0,\n",
    "    4: 5.5,\n",
    "    5: 6.0,\n",
    "    6: 6.5,\n",
    "    7: 7.0,\n",
    "    8: 7.5,\n",
    "    9: 8.0,\n",
    "    10: 8.5,\n",
    "    11: 9.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_prompt_version_RAG(topic, essay):\n",
    "    \n",
    "    results = search_cosine_similarity(f\"Prompt: {topic}\\nEssay: {essay}\", top_k=3)\n",
    "\n",
    "    grader_prompt = f\"\"\"\n",
    "    You are an IELTS writing section examiner. \n",
    "    Given the writing queston and the student essay, please grade the essay on a scale of 0 to 9 based on the IELTS Rubric and 0.5 intervals are allowed.\n",
    "\n",
    "    Writing Question: {topic}\n",
    "    Student Essay: {essay}\n",
    "\n",
    "    Here is an IELTS rubric for your reference: \n",
    "    Rubric: \n",
    "    {rubric.BASIC_RUBRIC}\n",
    "    {rubric.CRITERIA}\n",
    "    {rubric.BAND_SCORE}\n",
    "\n",
    "    Here is some similar paper along with their score for your reference:\n",
    "    Papar 1: Prompt: {results[0]['prompt']} Essay:  {results[0]['essay']} Score: {label_map[float(results[0]['label'])]}\n",
    "    Papar 2: Prompt: {results[1]['prompt']} Essay:  {results[1]['essay']} Score: {label_map[float(results[1]['label'])]}\n",
    "    Papar 3: Prompt: {results[2]['prompt']} Essay:  {results[2]['essay']} Score: {label_map[float(results[2]['label'])]}\n",
    "\n",
    "    Please output the score of the essay in the form of 'score of the essay'. Please output the score directly.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": grader_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_with_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        try:\n",
    "            if pred == '<4' or truth == '<4':\n",
    "                if pred == truth:\n",
    "                    correct += 1\n",
    "            elif float(pred) == float(truth) or abs(float(pred) - float(truth)) == 0.5:\n",
    "                correct += 1\n",
    "        except:\n",
    "            print(f\"Conversion Error, Skipping pred: {pred}, truth: {truth}\")\n",
    "            continue\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def get_acc_no_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        try:\n",
    "            if pred == '<4' or truth == '<4':\n",
    "                if pred == truth:\n",
    "                    correct += 1\n",
    "            elif float(pred) == float(truth):\n",
    "                correct += 1\n",
    "        except:\n",
    "            print(f\"Conversion Error, Skipping pred: {pred}, truth: {truth}\")\n",
    "            continue\n",
    "    return correct / total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.0', '5.5', '5.5', '6.0', '7.5', '5.0', '4.5', '7.0', '6.0', 'Score of the essay: 7.0', '7.0', '7.0', '5.0', '5.5']\n",
      "0     5.0\n",
      "1     5.5\n",
      "2     5.5\n",
      "3     5.0\n",
      "4     7.5\n",
      "5     6.5\n",
      "6     4.5\n",
      "7     7.0\n",
      "8     6.0\n",
      "9     7.0\n",
      "10    7.5\n",
      "11    7.0\n",
      "12    6.0\n",
      "13    5.5\n",
      "Name: predicted, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "truth_list = predicted_grade\n",
    "pred_list = []\n",
    "\n",
    "for (q,e) in zip(topic, essay):\n",
    "    pred_list.append(get_score_prompt_version_RAG(q, e))\n",
    "\n",
    "print(pred_list)\n",
    "print(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.0', '5.0', '6.5', '7.5', '5.0', '6.5', '6.0', '7.0', '6.5', '6.5', '7.0', '6.5', '7.0', '6.5', 'Score of the essay: 4.0', '6.0', '5.0', '6.0', '8.0', '6.0']\n",
      "[6.0, '<4', 6.5, 7.0, 4.0, 7.0, 7.0, 7.5, '<4', 7.5, 7.0, 5.0, 7.0, 7.5, 5.0, 9.0, 5.5, 7.5, 6.5, 5.5]\n"
     ]
    }
   ],
   "source": [
    "truth_list = []\n",
    "pred_list = []\n",
    "cnt = 0\n",
    "with open(\"../RAG/embeddings_dataset_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_test = json.load(f)\n",
    "\n",
    "for i in range(100,120):\n",
    "    curr_topic = metadata_test[i]['prompt']\n",
    "    curr_essay = metadata_test[i]['essay']\n",
    "    truth_list.append(label_map[float(metadata_test[i]['label'])])\n",
    "    pred_list.append(get_score_prompt_version_RAG(curr_topic, curr_essay))\n",
    "    cnt += 1\n",
    "    if cnt == 20:\n",
    "        break\n",
    "\n",
    "print(pred_list)\n",
    "print(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== For predicting essay score from the dataset ====\n",
      "Conversion Error, Skipping pred: Score of the essay: 4.0, truth: 5.0\n",
      "accuracy with no tolerace: 0.2\n",
      "Conversion Error, Skipping pred: Score of the essay: 4.0, truth: 5.0\n",
      "accuracy with 0.5 tolerace: 0.45\n",
      "\n",
      " ==== For predicting sample essay score from the desired score for essays in the dataset ====\n",
      "accuracy with no tolerace: 0.5714285714285714\n",
      "accuracy with 0.5 tolerace: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==== For predicting essay score from the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(pred_list, truth_list)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(pred_list, truth_list)}\\n\")\n",
    "\n",
    "print(\" ==== For predicting sample essay score from the desired score for essays in the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(sample_grade, desired_grade)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(sample_grade, desired_grade)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.0', '5.5', '6.5', '7.5', '5.5', '6.5', '5.5', '7.5', '7.5', '6.5', '7', '6.0', '8.0', '6.5', '3.5', '6.5', '5.5', '7.0', '8.5', '5.5']\n",
      "[6.0, '<4', 6.5, 7.0, 4.0, 7.0, 7.0, 7.5, '<4', 7.5, 7.0, 5.0, 7.0, 7.5, 5.0, 9.0, 5.5, 7.5, 6.5, 5.5]\n"
     ]
    }
   ],
   "source": [
    "truth_list = []\n",
    "pred_list = []\n",
    "cnt = 0\n",
    "with open(\"../RAG/embeddings_dataset_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_test = json.load(f)\n",
    "\n",
    "for i in range(100,120):\n",
    "    curr_topic = metadata_test[i]['prompt']\n",
    "    curr_essay = metadata_test[i]['essay']\n",
    "    truth_list.append(label_map[float(metadata_test[i]['label'])])\n",
    "    pred_list.append(get_score_prompt_version(curr_topic, curr_essay))\n",
    "    cnt += 1\n",
    "    if cnt == 20:\n",
    "        break\n",
    "\n",
    "print(pred_list)\n",
    "print(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== For predicting essay score from the dataset ====\n",
      "accuracy with no tolerace: 0.3\n",
      "accuracy with 0.5 tolerace: 0.45\n",
      "\n",
      " ==== For predicting sample essay score from the desired score for essays in the dataset ====\n",
      "accuracy with no tolerace: 0.5714285714285714\n",
      "accuracy with 0.5 tolerace: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==== For predicting essay score from the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(pred_list, truth_list)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(pred_list, truth_list)}\\n\")\n",
    "\n",
    "print(\" ==== For predicting sample essay score from the desired score for essays in the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(sample_grade, desired_grade)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(sample_grade, desired_grade)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
