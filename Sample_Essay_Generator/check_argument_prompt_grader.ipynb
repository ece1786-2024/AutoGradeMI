{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from Feedback_agent.rubric_and_sample import IELTS_rubrics as rubric\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_prompt_version(topic, essay):\n",
    "    client = OpenAI()\n",
    "    grader_prompt = f\"\"\"\n",
    "    You are an IELTS writing section examiner, tasked with evaluating essays strictly based on the official IELTS writing rubric.\n",
    "\n",
    "    **Writing Question**: {topic}\n",
    "    **Student Essay**: {essay}\n",
    "    \\n\n",
    "    **Reference IELTS Rubric**:\n",
    "    {rubric.BASIC_RUBRIC}\n",
    "    {rubric.CRITERIA}\n",
    "    {rubric.BAND_SCORE}\n",
    "    \\n\n",
    "    **Guidelines for Scoring**:\n",
    "    - Assign scores in 0.5 intervals from 0 to 9 based on the IELTS rubric.\n",
    "    - If the essay content is irrelevant or off-topic, assign a score of 0.\n",
    "    - Avoid generic scores like 5, 6, or 7 unless the essay fully justifies such a rating.\n",
    "    - Use the provided example essays and their scores to guide your grading and ensure consistency.\n",
    "\n",
    "    **Enhanced Scoring Process**:\n",
    "    - Generate multiple scores for the same essay by slightly varying the context or examples provided (e.g., shuffle or modify reference essays where appropriate).\n",
    "    - Compute the average of these scores to improve reliability.\n",
    "    - If scores vary significantly, consider revisiting the rubric alignment for the given essay.\n",
    "\n",
    "    **Final Output**:\n",
    "    - Output only the final averaged score directly, only the score number, if the score is smaller than 4, output only the '<4' instead of the score.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": grader_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = faiss.read_index(\"../RAG/faiss_index_train.bin\")\n",
    "index = faiss.read_index(\"../RAG/faiss_index_train_topics.bin\")\n",
    "#with open(\"../RAG/embeddings_dataset_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "with open(\"../RAG/embeddings_dataset_train_topics.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cosine_similarity(query_text, top_k=3):\n",
    "    \n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding_np)\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = metadata[idx]\n",
    "        result[\"similarity\"] = distances[0][i]  \n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: '<4',\n",
    "    1: 4.0,\n",
    "    2: 4.5,\n",
    "    3: 5.0,\n",
    "    4: 5.5,\n",
    "    5: 6.0,\n",
    "    6: 6.5,\n",
    "    7: 7.0,\n",
    "    8: 7.5,\n",
    "    9: 8.0,\n",
    "    10: 8.5,\n",
    "    11: 9.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reference: https://ielts.idp.com/canada/prepare/article-ielts-writing-task-2-8-steps-to-band-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_prompt_version_RAG(topic, essay):\n",
    "    \n",
    "    results = search_cosine_similarity(f\"Prompt: {topic}\\nEssay: {essay}\", top_k=3)\n",
    "\n",
    "    grader_prompt = f\"\"\"\n",
    "    You are a professional IELTS writing section examiner, tasked to grade the student essay accurately based on the official IELTS writing rubric, but there should be some space for leniency.\n",
    "\n",
    "    **Writing Question**: {topic}\n",
    "    **Student Essay**: {essay}\n",
    "    \\n\n",
    "    **Reference IELTS Rubric**:\n",
    "    {rubric.BASIC_RUBRIC}\n",
    "    {rubric.CRITERIA}\n",
    "    {rubric.BAND_SCORE}\n",
    "    \\n\n",
    "    **Reference Essays for Consistency**:\n",
    "    - Example 1: Writing Question: {results[0]['prompt']} Essay: {results[0]['essay']} Score: {label_map[float(results[0]['label'])]}\n",
    "    \\n\n",
    "    - Example 2: Writing Question: {results[1]['prompt']} Essay: {results[1]['essay']} Score: {label_map[float(results[1]['label'])]}\n",
    "    \\n\n",
    "    - Example 3: Writing Question: {results[2]['prompt']} Essay: {results[2]['essay']} Score: {label_map[float(results[2]['label'])]}\n",
    "    \\n\n",
    "\n",
    "    **Guidelines for Scoring**:\n",
    "    - Assign scores in 0.5 intervals from 0 to 9 based on the IELTS rubric.\n",
    "    - If the student essay is irrelevant or off-topic from the Writing Question, assign a score of 0.\n",
    "    - Avoid generic scores like 5, 6, or 7 unless the student essay fully justifies such a rating.\n",
    "    - Use the provided example essays and their scores to guide your grading and ensure consistency.\n",
    "\n",
    "    **Special Consideration for Student Essays to Achieve High Scores**:\n",
    "    - The student essay should be a formal response to the writing question. \n",
    "    - Make sure the student essay is logical and progresses clearly with a wide range of linking words and phrases. But the student essay should avoid an overuse of the same linking words. \n",
    "    - Check for repetitions of words, the student essay should cover a diverse range of vocabularies. \n",
    "    - The ideas MUST be organised into paragraphs with introductions of arguments, examples that support the student essay's viewpoint, explanations of why these examples are valid, and great transitions to the next topic or paragraph. \n",
    "    - The student essay should have sufficient amount of paragraphs to show structured response. \n",
    "    - Each topic or argument should have its own paragraph. The introduction and the conclusion should also be included as separated paragraphs. \n",
    "    - The student essay should use a wide range of vocabularies and an adequate amount of uncommon words. \n",
    "    - The student essay should use a wide range of sentence structures with accurate punctuation. The student essay should contain a variety of simple and complex sentence structures. \n",
    "\n",
    "    **Enhanced Scoring Process**:\n",
    "    - Generate multiple scores for the same student essay by slightly varying the context or examples provided (e.g., shuffle or modify reference essays where appropriate).\n",
    "    - Compute the average of these scores to improve reliability.\n",
    "    - If scores vary significantly, consider revisiting the rubric alignment for the given student essay.\n",
    "    - A high scoring student essay (scores ranging from 7.5 to 9) DO NOT need to fulfil all the requirements in \"Special Consideration for Student Essays to Achieve High Scores\" section. \n",
    "\n",
    "    **Final Output**:\n",
    "    - Output only the final averaged score directly, only the score number, but if the score is smaller than 4, output only the '<4'.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": grader_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_with_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        if '<4' in pred:\n",
    "            pred = \"<4\"\n",
    "        elif float(pred) <= 4:\n",
    "            pred = '<4'\n",
    "        try:\n",
    "            if pred == '<4' or truth == '<4':\n",
    "                if pred == truth:\n",
    "                    correct += 1 \n",
    "            elif float(pred) == float(truth) or abs(float(pred) - float(truth)) == 0.5:\n",
    "            #elif float(pred) == float(truth) or abs(float(pred) - float(truth)) <= 1.0:\n",
    "                correct += 1\n",
    "        except:\n",
    "            total -= 1\n",
    "            print(f\"Conversion Error, Skipping pred: {pred}, truth: {truth}\")\n",
    "            continue\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def get_acc_no_tol(pred_list, truth_list):\n",
    "    total = len(truth_list)\n",
    "    correct = 0\n",
    "    for (pred, truth) in zip(pred_list, truth_list):\n",
    "        if '<4' in pred:\n",
    "            pred = \"<4\"\n",
    "        elif float(pred) < 4:\n",
    "            pred = '<4'\n",
    "        try:\n",
    "            if pred == '<4' or truth == '<4':\n",
    "                if pred == truth:\n",
    "                    correct += 1\n",
    "            elif float(pred) == float(truth):\n",
    "                correct += 1\n",
    "        except:\n",
    "            total -= 1\n",
    "            print(f\"Conversion Error, Skipping pred: {pred}, truth: {truth}\")\n",
    "            continue\n",
    "    return correct / total\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.5', '6.0', '6.5', '5.5', '5.0', '5.0', '6.5', '5.5', '<4', '7.0', '7.0', '5.5', '<4', '0', '6.5', '5.5', '6.0', '6.0', '7.0', '6.0', '5.5', '7.0', '<4', '6.0', '6.0', '6.0', '6.5', '5.0', '8.0', '<4', '7.5', '7.5', '6.0', '6.5', '5.0', '5.0', '6.5', '6.5', '<4', '6.5']\n",
      "[7.5, 7.0, 9.0, 6.5, 6.5, 5.0, 6.5, 6.0, '<4', 8.0, 5.5, 5.0, '<4', '<4', 6.5, 5.0, 7.5, 6.0, 4.5, 5.0, 5.0, 7.5, 7.5, 8.0, 6.0, 7.0, 7.5, 6.0, 7.5, '<4', 6.0, 8.0, 7.5, 6.0, 4.0, 7.5, 6.0, 7.5, '<4', 7.0]\n"
     ]
    }
   ],
   "source": [
    "truth_list = []\n",
    "pred_RAG_list = []\n",
    "pred_prompt_list = []\n",
    "cnt = 0\n",
    "with open(\"../RAG/manual_embedding_dataset_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_test = json.load(f)\n",
    "\n",
    "for i in range(40):\n",
    "    curr_topic = metadata_test[i]['prompt']\n",
    "    curr_essay = metadata_test[i]['essay']\n",
    "    truth_list.append(label_map[float(metadata_test[i]['label'])])\n",
    "    pred_RAG_list.append(get_score_prompt_version_RAG(curr_topic, curr_essay))\n",
    "    cnt += 1\n",
    "    # if cnt == 10:\n",
    "    #    break\n",
    "\n",
    "print(pred_RAG_list)\n",
    "print(truth_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== For predicting essay score from the dataset ====\n",
      "accuracy with no tolerace: 0.25\n",
      "accuracy with 0.5 tolerace: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==== For predicting essay score from the dataset ====\")\n",
    "print(f\"accuracy with no tolerace: {get_acc_no_tol(pred_RAG_list, truth_list)}\")\n",
    "print(f\"accuracy with 0.5 tolerace: {get_acc_with_tol(pred_RAG_list, truth_list)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
