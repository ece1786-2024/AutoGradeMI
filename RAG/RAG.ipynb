{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 9766\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='..\\data\\dataset\\processed\\clean_data_gpt2.csv')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the train and test part for 9:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 8789\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start to create embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=texts, model=model)\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings for batch: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 88/88 [03:27<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "metadata = []\n",
    "\n",
    "\n",
    "batch_size = 100  \n",
    "\n",
    "for i in tqdm(range(0, len(split_dataset['train']), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch_samples = split_dataset['train'][i:i+batch_size]\n",
    "\n",
    "    \n",
    "    \n",
    "    combined_texts = [\n",
    "        f\"Prompt: {prompt}\\nEssay: {essay}\"\n",
    "        for prompt, essay in zip(batch_samples['prompt'], batch_samples['essay'])\n",
    "    ]\n",
    "    \n",
    "    batch_embeddings = generate_embeddings_batch(combined_texts)\n",
    "    if batch_embeddings is not None:\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        metadata.extend([\n",
    "            {\"prompt\": prompt, \"essay\": essay, \"label\": label}\n",
    "            for prompt, essay, label in zip(\n",
    "                batch_samples['prompt'], batch_samples['essay'], batch_samples['label']\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "\n",
    "# save the embeddings\n",
    "\n",
    "embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "faiss.normalize_L2(embeddings_np)  \n",
    "\n",
    "dimension = len(embeddings_np[0])  \n",
    "index = faiss.IndexFlatIP(dimension)  \n",
    "index.add(embeddings_np)  \n",
    "\n",
    "# save the reults\n",
    "faiss.write_index(index, \"faiss_index_train.bin\")\n",
    "\n",
    "\n",
    "with open(\"embeddings_dataset_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  Prompt: some people believe that eventually all jobs will be done by artificially intelligence robots.\n",
      "what is your opinions?\n",
      "  Essay: Hello dear, Long time, am happy to hear from you, because even me i have missed you too, Since you left i been lonely even to work around in the evening as we used to do, going to church, market and so on, it has not been easy for me, Hearing from you show me that our relationship is still intact. Based on your request, i have told you time without number that anywhere you find yourself try to adapt, i know it will not going to be easy with two of us but you have to same to me. I will suggest for you to join school club, it will keep you busy both soul and body, you know how student life is at list every Friday is club neither in the school environment or outside the school, apart from joining club, try to engage yourself also in school choir which i know you know how to sing very well, While singing i believe you will forget some of our movement and sleepover we used to do. So also to be participating in some school activities like playing valley ball, which you know that you have the height, so try to make use of your height in other to make your self happy, playing with one or two people in the valley ball pitch one day you will make a new friend, and may be you may not even remember again. Last i will try to come and see you by next week. I wish you well see you then bye.\n",
      "  Label: 1\n",
      "  Similarity: 0.9954447746276855\n",
      "\n",
      "Result 2:\n",
      "  Prompt: Some people believe that eventually all jobs will be done by artificial intelligence robots. What is your opinion?\n",
      "  Essay: As development of technology grows, many useful products have been invented. One of the greatest inventions is the artificial intelligence robot. They freed our human’s hands. Additional, they also have higher productivity than human beings. However, artificially robots cannot replace occupations that required human interaction, knowledge, engagement and etc. In this essay, I will talk about my opinions.\n",
      "\n",
      "Nowadays, A.I. technology is already very mature. Robots can work in a dangerous environment, avoid accidents, work in a longer span and etc. For instance, when there is radiation in the work place, robotic automation is the ideal solution because they can work automatically without getting illness and injuries. Furthermore, breaks are not necessary for them. They can work twenty- four hours a day and seven days a week without any rest. All they need is electricity to supply them.\n",
      "\n",
      "On the other hand, they also have many flaws, such as lack of emotion, lack of creativity and etc. Therefore, they cannot work as a psychologist. This job required empathy in order to help the clients out of the predicament. Moreover, they can only work for the programmed or given instruction. It will not think out of the box like human and it might give unexpected output.\n",
      "\n",
      "In conclusion, there are still some jobs cannot replace by A.I. robots. They need improvement on interaction, emotion and creativity. As long as these weaknesses being improved, it could be a major part of the society.\n",
      "  Label: 2\n",
      "  Similarity: 0.9036785960197449\n",
      "\n",
      "Result 3:\n",
      "  Prompt: Some people believe that eventually all jobs will be done by artificially intelligent robots.\n",
      "What is your Opinion?\n",
      "  Essay: In the forthcoming future, some individuals believe that artificial intelligence would develop all kinds of work, in a job occupation. \n",
      "\n",
      "Nowadays we are facing the increase of artificial intelligence as never before. As a consequence, a plethora of activities and task has, more and more, been developed by it in diverse areas of our life. To exemplify, there are shops that have robots to clean the place and also to work in the place of an employee as a salesman. Furthermore, robots are already cleaning the houses of ordinary people and this reality is becoming more popular around the world. In my city, for instance, supermarkets are offering several models of this machine. Moreover, there are machines developing tasks in huge enterprises like the vehicle industry and, as a consequence, an immense quantity of people was fired.\n",
      "\n",
      "On the other hand, from my perspective, there are some activities that could never be developed by a machine. For instance, the role of a teacher in their students lives. In this case, a robot could teach for hours without stop it. However, it is not capable to give a snuggly hug to a child to calm her down. As a result, artificial intelligence could not fit in that sort of tasks. In addition, a teacher has the capacity of being emphatic with their students and it is another criterion that does not belong to robots and it is crucial for a remarkable performance at school because children and adolescents need to be understood, need to talk with real people in order to understand themselves and achieve their goals.\n",
      "\n",
      "In conclusion, from my experience, activities done by artificially intelligent robots could bring us a plethora of benefits on our daily basis, such as do faster shopping at a restaurant or mall. Although, it is necessary to consider that some jobs are better developed by humans due to the fact that we have emotions and feeling, which are not part of a robot. Thus, the properly roles need to be developed wisely by and for the society.\n",
      "  Label: 8\n",
      "  Similarity: 0.903133749961853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss_index_train.bin\")\n",
    "with open(\"embeddings_dataset_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "\n",
    "def search_cosine_similarity(query_text, top_k=3):\n",
    "    \n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding_np)\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = metadata[idx]\n",
    "        result[\"similarity\"] = distances[0][i]  \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "query = \"\"\"Prompt: some people believe that eventually all jobs will be done by artificially intelligence robots.\n",
    "what is your opinions?\n",
    "  Essay: Hello dear, Long time, am happy to hear from you, because even me i have missed you too, \n",
    "  Since you left i been lonely even to work around in the evening as we used to do, going to church, market and so on, \n",
    "  it has not been easy for me, Hearing from you show me that our relationship is still intact. Based on your request, \n",
    "  i have told you time without number that anywhere you find yourself try to adapt, i know it will not going to be easy with two of us but you have to same to me. \n",
    "  I will suggest for you to join school club, it will keep you busy both soul and body, you know how student life is at list every Friday is club neither in the school \n",
    "  environment or outside the school, apart from joining club, try to engage yourself also in school choir which i know you know how to sing very well, While singing i \n",
    "  believe you will forget some of our movement and sleepover we used to do. So also to be participating in some school activities like playing valley ball, which you know \n",
    "  that you have the height, so try to make use of your height in other to make your self happy, playing with one or two people in the valley ball pitch one day you will make a new friend, \n",
    "  and may be you may not even remember again. Last i will try to come and see you by next week. I wish you well see you then bye.\n",
    "\"\"\"\n",
    "results = search_cosine_similarity(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Prompt: {result['prompt']}\")\n",
    "    print(f\"  Essay: {result['essay']}\")\n",
    "    print(f\"  Label: {result['label']}\")\n",
    "    print(f\"  Similarity: {result['similarity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  15%|█▌        | 1359/8789 [04:35<19:43,  6.28it/s]  "
     ]
    }
   ],
   "source": [
    "# embeddings = []\n",
    "# metadata = []\n",
    "# for sample in tqdm(split_dataset['train'], desc=\"Generating embeddings\"):\n",
    "#     combined_text = f\"Prompt: {sample['prompt']}\\nEssay: {sample['essay']}\"\n",
    "    \n",
    "#     embedding = generate_embedding(combined_text)\n",
    "#     if embedding is not None:\n",
    "#         embeddings.append(embedding)\n",
    "#         metadata.append({\n",
    "#             \"prompt\": sample[\"prompt\"],\n",
    "#             \"essay\": sample[\"essay\"],\n",
    "#             \"label\": sample[\"label\"]\n",
    "#         })\n",
    "\n",
    "                         \n",
    "\n",
    "# # save the embeddings\n",
    "\n",
    "# embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "# faiss.normalize_L2(embeddings_np)  \n",
    "\n",
    "# dimension = len(embeddings_np[0])  \n",
    "# index = faiss.IndexFlatIP(dimension)  \n",
    "# index.add(embeddings_np)  \n",
    "\n",
    "# # save the reults\n",
    "# faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "\n",
    "# with open(\"embeddings_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "Accuracy: 0.18666666666666668\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "index = faiss.read_index(\"faiss_index_train.bin\")\n",
    "with open(\"embeddings_dataset_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_test = json.load(f)\n",
    "\n",
    "with open(\"embeddings_dataset_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_train = json.load(f)\n",
    "\n",
    "def search_cosine_similarity(query_text, dataset, top_k=3):\n",
    "    \n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding_np)\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = dataset[idx]\n",
    "        result[\"similarity\"] = distances[0][i]  \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "correct = 0\n",
    "cnt = 0\n",
    "timeout_duration = 10\n",
    "for data in metadata_test:\n",
    "    query = f\"Prompt: {data['prompt']}\\nEssay: {data['essay']}\"\n",
    "    results = search_cosine_similarity(query, metadata_train, top_k=2)\n",
    "\n",
    "    answer = float(data['label'])\n",
    "    curr_score = 0\n",
    "    weight = [0.8, 0.2]\n",
    "    for i, result in enumerate(results):\n",
    "        curr_score += result['label']*weight[i]\n",
    "\n",
    "    # print(curr_score)\n",
    "    print(cnt)\n",
    "\n",
    "\n",
    "    if (abs(answer - curr_score) <= 1):\n",
    "        correct += 1\n",
    "    \n",
    "    cnt += 1\n",
    "    if cnt == 150:\n",
    "        break\n",
    "    # print(\"=====================\")\n",
    "\n",
    "print(f\"Accuracy: {correct/cnt}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
