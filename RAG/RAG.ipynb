{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 9766\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='..\\data\\dataset\\processed\\clean_data_gpt2.csv')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the train and test part for 9:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 8789\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'essay', 'label'],\n",
      "        num_rows: 977\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start to create embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        response = client.embeddings.create(input=texts, model=model)\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings for batch: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 88/88 [03:27<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "metadata = []\n",
    "\n",
    "\n",
    "batch_size = 100  \n",
    "\n",
    "for i in tqdm(range(0, len(split_dataset['train']), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch_samples = split_dataset['train'][i:i+batch_size]\n",
    "\n",
    "    \n",
    "    \n",
    "    combined_texts = [\n",
    "        f\"Prompt: {prompt}\\nEssay: {essay}\"\n",
    "        for prompt, essay in zip(batch_samples['prompt'], batch_samples['essay'])\n",
    "    ]\n",
    "    \n",
    "    batch_embeddings = generate_embeddings_batch(combined_texts)\n",
    "    if batch_embeddings is not None:\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        metadata.extend([\n",
    "            {\"prompt\": prompt, \"essay\": essay, \"label\": label}\n",
    "            for prompt, essay, label in zip(\n",
    "                batch_samples['prompt'], batch_samples['essay'], batch_samples['label']\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "\n",
    "# save the embeddings\n",
    "\n",
    "embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "faiss.normalize_L2(embeddings_np)  \n",
    "\n",
    "dimension = len(embeddings_np[0])  \n",
    "index = faiss.IndexFlatIP(dimension)  \n",
    "index.add(embeddings_np)  \n",
    "\n",
    "# save the reults\n",
    "faiss.write_index(index, \"faiss_index_train.bin\")\n",
    "\n",
    "\n",
    "with open(\"embeddings_dataset_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  Prompt: some people believe that eventually all jobs will be done by artificially intelligence robots.\n",
      "what is your opinions?\n",
      "  Essay: Hello dear, Long time, am happy to hear from you, because even me i have missed you too, Since you left i been lonely even to work around in the evening as we used to do, going to church, market and so on, it has not been easy for me, Hearing from you show me that our relationship is still intact. Based on your request, i have told you time without number that anywhere you find yourself try to adapt, i know it will not going to be easy with two of us but you have to same to me. I will suggest for you to join school club, it will keep you busy both soul and body, you know how student life is at list every Friday is club neither in the school environment or outside the school, apart from joining club, try to engage yourself also in school choir which i know you know how to sing very well, While singing i believe you will forget some of our movement and sleepover we used to do. So also to be participating in some school activities like playing valley ball, which you know that you have the height, so try to make use of your height in other to make your self happy, playing with one or two people in the valley ball pitch one day you will make a new friend, and may be you may not even remember again. Last i will try to come and see you by next week. I wish you well see you then bye.\n",
      "  Label: 1\n",
      "  Similarity: 0.7499831914901733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss_index_train.bin\")\n",
    "with open(\"embeddings_dataset_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "\n",
    "def search_cosine_similarity(query_text, top_k=3):\n",
    "    \n",
    "    query_embedding = generate_embedding(query_text)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "    query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
    "    faiss.normalize_L2(query_embedding_np)\n",
    "    \n",
    "    \n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        result = metadata[idx]\n",
    "        result[\"similarity\"] = distances[0][i]  \n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "query = \"\"\"Prompt: some people believe that eventually all jobs will be done by artificially intelligence robots.\n",
    "what is your opinions?\n",
    "  Essay: Hello dear, Long time, am happy to hear from you, because even me i have missed you too, \n",
    "  Since you left i been lonely even to work around in the evening as we used to do, going to church, market and so on, \n",
    "  it has not been easy for me, Hearing from you show me that our relationship is still intact. Based on your request, \n",
    "  i have told you time without number that anywhere you find yourself try to adapt, i know it will not going to be easy with two of us but you have to same to me. \n",
    "  I will suggest for you to join school club, it will keep you busy both soul and body, you know how student life is at list every Friday is club neither in the school \n",
    "  environment or outside the school, apart from joining club, try to engage yourself also in school choir which i know you know how to sing very well, While singing i \n",
    "  believe you will forget some of our movement and sleepover we used to do. So also to be participating in some school activities like playing valley ball, which you know \n",
    "  that you have the height, so try to make use of your height in other to make your self happy, playing with one or two people in the valley ball pitch one day you will make a new friend, \n",
    "  and may be you may not even remember again. Last i will try to come and see you by next week. I wish you well see you then bye.\n",
    "\"\"\"\n",
    "results = search_cosine_similarity(query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"  Prompt: {result['prompt']}\")\n",
    "    print(f\"  Essay: {result['essay']}\")\n",
    "    print(f\"  Label: {result['label']}\")\n",
    "    print(f\"  Similarity: {result['similarity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:  15%|█▌        | 1359/8789 [04:35<19:43,  6.28it/s]  "
     ]
    }
   ],
   "source": [
    "# embeddings = []\n",
    "# metadata = []\n",
    "# for sample in tqdm(split_dataset['train'], desc=\"Generating embeddings\"):\n",
    "#     combined_text = f\"Prompt: {sample['prompt']}\\nEssay: {sample['essay']}\"\n",
    "    \n",
    "#     embedding = generate_embedding(combined_text)\n",
    "#     if embedding is not None:\n",
    "#         embeddings.append(embedding)\n",
    "#         metadata.append({\n",
    "#             \"prompt\": sample[\"prompt\"],\n",
    "#             \"essay\": sample[\"essay\"],\n",
    "#             \"label\": sample[\"label\"]\n",
    "#         })\n",
    "\n",
    "                         \n",
    "\n",
    "# # save the embeddings\n",
    "\n",
    "# embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "# faiss.normalize_L2(embeddings_np)  \n",
    "\n",
    "# dimension = len(embeddings_np[0])  \n",
    "# index = faiss.IndexFlatIP(dimension)  \n",
    "# index.add(embeddings_np)  \n",
    "\n",
    "# # save the reults\n",
    "# faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "\n",
    "# with open(\"embeddings_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece1786",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
